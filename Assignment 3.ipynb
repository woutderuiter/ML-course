{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment 3\n",
    "\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as).\n",
    "\n",
    " **Deadline:** Thursday, March 29,  2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "\n",
    "import sklearn.decomposition as deco\n",
    "import sklearn.manifold as manifold\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Isomap (5 Points, 1+2+2)\n",
    "\n",
    "Apply PCA and Isomap to images of handwritten digits (see below). You may use sklearn.decomposition and sklearn.manifold.\n",
    "\n",
    "### a)  \n",
    "Compute the first two components of the data using PCA. Make a scatter plot of the data in the first two components of PCA indicating class with color.\n",
    "\n",
    "### b)\n",
    " Compute an Isomap embedding with two components with nr_neighbors={5, 50, N-1} (three separate embeddings).\n",
    " For each of the Isomap embeddings, apply the function \"align\" (see below) with \"ref_data\" as your computed pca embedding and \"data\" as the isomap embedding. Show a scatter plot of each of the aligned isomap embeddings.\n",
    " \n",
    "### c)\n",
    "\n",
    "Visually compare how well the classes are separated in the different scatter plots. What is the effect of changing the number of neighbors on the score computed in the alignment function? What does it mean if the score is zero? When do you expect the score to become zero and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "N=len(X)\n",
    "\n",
    "# Align a data set with a reference data set minimizing l_1 error\n",
    "# Returns aligned data set and alignment error\n",
    "def align(ref_data, data):\n",
    "    \n",
    "    transformations = np.asarray([\n",
    "        [[0,1],[1,0]], \n",
    "        [[0,-1],[1,0]], \n",
    "        [[0,1],[-1,0]], \n",
    "        [[0,-1],[-1,0]], \n",
    "        [[1,0],[0,1]], \n",
    "        [[1,0],[0,-1]], \n",
    "        [[-1,0],[0,1]], \n",
    "        [[-1,0],[0,-1]] \n",
    "    ])\n",
    "    \n",
    "    score = []\n",
    "    for i in range(0,8):\n",
    "        transf_data =   np.matmul(data, transformations[i])\n",
    "        score.append(np.linalg.norm( transf_data - ref_data, ord=1) )\n",
    "        \n",
    "    idx = np.argmin(score)\n",
    "    transf_data = np.matmul(data,transformations[idx])\n",
    "    \n",
    "    print(\"Aligned the data sets. Score is {0:10.1f}  \".format(score[idx]))\n",
    "    \n",
    "    return transf_data, score[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Multidimensional Scaling (6 Points, 1+2+2+1)\n",
    "\n",
    "Show that for mean-centered data sets we can recover inner products using\n",
    "pairwise distance information only. This is used by the isomap embedding algorithm.\n",
    "\n",
    "We are given all squared pairwise distances of an otherwise unknown\n",
    "point set ${\\bf p}_1,\\dots, {\\bf p}_n \\in \\mathbb{R}^d$, i.e., we are given \n",
    "for all $1 \\leq i,j \\leq n$ the values\n",
    "\n",
    "$$  d_{ij} = \\|{\\bf p}_i - {\\bf p}_j\\|^2. $$\n",
    "\n",
    "We assume that the point set is mean-centered, that is \n",
    "\n",
    "$$ \\sum_{i=1}^{n} {\\bf p}_i = \\vec{{\\bf 0}}.$$\n",
    "\n",
    "(where $\\vec{{\\bf 0}}$ is the vector of zeros)\n",
    "\n",
    "\n",
    "In the following, $\\langle {\\bf p}_i , {\\bf p}_j \\rangle$ denotes the inner \n",
    "product of ${\\bf p}_i$ and ${\\bf p}_j$. \n",
    "Prove that the following holds true for mean-centered point sets: \n",
    "\n",
    "$$-2 \\langle {\\bf p}_i , {\\bf p}_j \\rangle = \n",
    "d_{ij}\n",
    "- \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n}   \n",
    "-  \\sum_{\\ell=1}^{n} \\frac{d_{j\\ell}}{n} \n",
    "+ \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{n^2} \n",
    "$$\n",
    "\n",
    "You may use the following steps in your derivation.\n",
    "\n",
    "### a) \n",
    "Expand  $d_{ij}$ to yield an expression of $\\langle {\\bf p}_i, {\\bf p}_j \\rangle$, $\\|{\\bf p}_i\\|^2$ and $\\|{\\bf p}_j\\|^2$.\n",
    "\n",
    "### b) \n",
    "Show that the following holds for any $ {\\bf q} \\in \\mathbb{R}^d$:\n",
    "$$ \\sum_{1 \\leq i \\leq n} \\langle {\\bf p}_i , {\\bf q} \\rangle =   0$$\n",
    "\n",
    "### c) \n",
    "Prove that \n",
    "$$ \\|{\\bf p}_i\\|^2 = \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{2n^2}$$\n",
    "\n",
    "### d) \n",
    "Combine the steps in your proof. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-sensitive hashing (4 Points, 2+1+1)\n",
    "\n",
    "### a) \n",
    "\n",
    "Prove that if the Jaccard Similarity of two sets is $0$, then minhashing always gives a correct estimate of the Jaccard similarity\n",
    "\n",
    "### b) \n",
    "Let $H$ be a family of $(d_1,d_2,p_1,p_2)$-locality-sensitive hash functions.\n",
    "Assume that $p_2=0$ and assume we have a total numer of $m$ hash\n",
    "functions from this family available.  Which combination of AND-constructions\n",
    "and OR-constructions should we use to maximally amplify the hash family?\n",
    "\n",
    "### (c) \n",
    "\n",
    "Let $H$ be a family of $(d_1,d_2,p_1,p_2)$-locality-sensitive hash functions.\n",
    "Assume that $p_2=\\frac{1}{n}$ and assume we have $n$ data points $\\bf P$\n",
    "which are stored in a hash table using a randomly chosen function $h$ from $H$.\n",
    "Given a query point $\\bf q$, we retrieve the points in the hash bucket with index $h(\\bf q)$ to search \n",
    "for a point which has small distance to $\\bf q$. \n",
    "Let $X$ be a random variable that is equal to the size of the set \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\{{\\bf p \\in P}~:~ h({\\bf p})=h({\\bf q}) ~\\wedge~ d({\\bf p,q}) \\geq d_2\\right\\}\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which consists of the false positives of this query.\n",
    "Derive the expected number of false-positives $E\\left[ X \\right]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
